apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 30m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: "65.x"
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
  values:
    securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
    # Prometheus configuration
    prometheus:
      prometheusSpec:
        retention: 7d
        storageSpec:
          volumeClaimTemplate:
            spec:
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 10Gi
        resources:
          requests:
            memory: 512Mi
          limits:
            memory: 2Gi
        nodeSelector:
          kubernetes.io/hostname: "home"
        tolerations:
          - key: "freebox"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"
    secrets:
        - discord-webhook
    # Grafana configuration
    grafana:
      enabled: true
      adminPassword: "admin" # Change this after first login!
      nodeSelector:
        kubernetes.io/hostname: "home"
      tolerations:
        - key: "freebox"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt-production
          traefik.ingress.kubernetes.io/router.entrypoints: websecure
        hosts:
          - grafana.erwanleboucher.dev
        tls:
          - secretName: grafana-tls
            hosts:
              - grafana.erwanleboucher.dev
      persistence:
        enabled: true
        size: 2Gi
      resources:
        requests:
          memory: 256Mi
        limits:
          memory: 512Mi

    # AlertManager configuration with Discord
    alertmanager:
      enabled: true
      alertmanagerSpec:
        nodeSelector:
          kubernetes.io/hostname: "home"

        resources:
          requests:
            memory: 128Mi
          limits:
            memory: 256Mi
      config:
        global:
          resolve_timeout: 5m
        route:
          group_by: ['alertname', 'cluster', 'service']
          group_wait: 10s
          group_interval: 10s
          repeat_interval: 12h
          receiver: 'discord'
          routes:
            - match:
                alertname: Watchdog
              receiver: 'null'
        receivers:
          - name: 'null'
          - name: 'discord'
            discord_configs:
              - webhook_url_file: '/etc/alertmanager/secrets/discord-webhook/webhook-url'
                title: '{{ template "discord.default.title" . }}'
                message: '{{ template "discord.default.message" . }}'
        templates:
          - '/etc/alertmanager/config/*.tmpl'
    kubelet:
      serviceMonitor:
        cAdvisorMetricRelabelings:
          # Drop heavy metrics that are rarely used in homelabs
          - sourceLabels: [__name__]
            action: drop
            regex: (container_network_.*|container_fs_.*|container_spec_.*|container_tasks_state|container_memory_failures_total)
          # Drop extremely verbose labels to reduce cardinality
          - action: labeldrop
            regex: (id|image_id|uid|docker_image)
    # Disable some default exporters to reduce resource usage
    nodeExporter:
      enabled: true
      serviceMonitor:
        relabelings:
          # Drop metrics for filesystems you don't care about (tmpfs, etc)
          - sourceLabels: [mountpoint]
            regex: /var/lib/kubelet/pods.+
            action: drop

    kubeStateMetrics:
      enabled: true
      nodeSelector:
        kubernetes.io/hostname: "home"
      tolerations:
        - key: "freebox"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"

    # Disable etcd and scheduler monitoring (not exposed in k3s by default)
    coreDns:
      enabled: false  # Often not needed if you don't debug DNS internals
    kubeApiServer:
      enabled: true   # Keep this, it's light
    kubeEtcd:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeControllerManager:
      enabled: false

    # Default alert rules
    defaultRules:
      create: true
      rules:
        alertmanager: true
        etcd: false
        configReloaders: true
        general: true
        k8s: true
        kubeApiserverAvailability: true
        kubeApiserverSlos: true
        kubelet: true
        kubeProxy: true
        kubePrometheusGeneral: true
        kubePrometheusNodeRecording: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        kubeScheduler: false
        kubeStateMetrics: true
        network: true
        node: true
        nodeExporterAlerting: true
        nodeExporterRecording: true
        prometheus: true
        prometheusOperator: true
    prometheus-node-exporter:
      hostRootFsMount:
        enabled: false
        mountPropagation: HostToContainer
    extraVolumes:
      - name: grafana-host-storage
        hostPath:
          path: /opt/yams/config/grafana
          type: DirectoryOrCreate

      # 3. Mount the volume to Grafana's data directory
    extraVolumeMounts:
        - name: grafana-host-storage
          mountPath: /var/lib/grafana
